{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"export/code/tdd-poem-algorithm3.gif\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout\n",
    "#from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 788921 characters\n",
      "Our corpus contains 141 unique characters.\n"
     ]
    }
   ],
   "source": [
    "artist = \"pushkin\"\n",
    "with open(f\"{artist}.txt\", encoding=\"utf-8\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "print(\"Loaded a corpus of {0} characters\".format(len(corpus)))\n",
    "\n",
    "# Get a unique identifier for each char in the corpus, then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "\n",
    "\n",
    "sentence_length = 50\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 788871 sentences of length 50\n",
      "Vectorizing X and y...\n",
      "Sanity check y. Dimension: (788871, 141) # Sentences: 788871 Characters in corpus: 141\n",
      "Sanity check X. Dimension: (788871, 50, 141) Sentence length: 50\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))\n",
    "\n",
    "# Vectorize our data and labels. We want everything in one-hot\n",
    "# because smart data encoding cultivates phronesis and virtue.\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\".format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "#model.add(CuDNNLSTM(256, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "#model.add(CuDNNLSTM(256))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'{artist}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.6):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(temperature):\n",
    "\n",
    "    start_index = random.randint(0, len(corpus) - sentence_length - 1)\n",
    "\n",
    "    speak = '<speak>'\n",
    "    sentence = corpus[start_index: start_index + sentence_length]\n",
    "\n",
    "    after_nl = False\n",
    "    for i in range(1000):\n",
    "        x_pred = np.zeros((1, sentence_length, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, encoding[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = decoding[next_index]\n",
    "      \n",
    "        sentence = sentence[1:] + next_char\n",
    "        if next_char == '\\n' and not after_nl:\n",
    "            after_nl = True\n",
    "            continue\n",
    "        if not after_nl:\n",
    "            continue\n",
    "        sys.stdout.write(next_char)\n",
    "        if next_char == '\\n':\n",
    "            speak += '<break time=\"30ms\"/> '\n",
    "        speak += next_char\n",
    "        sys.stdout.flush()\n",
    "        if next_char == '.':\n",
    "            break\n",
    "    return speak + '</speak>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Братии сосне томим, забот,\n",
      "Своей на встречу за веселием,\n",
      "Я даже славно за душа моим."
     ]
    }
   ],
   "source": [
    "result = generate(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним текст для Алексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_for_alexa(text):\n",
    "    with open('alexa.txt', 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_alexa(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
